#!/usr/bin/env python3
"""
sdk/exp3_claim3_optionB_runner.py

Framework v4.5 Research Framework Specification (FRAMEWORK_SPEC_v0.2.md)
Claim 3 — Option B runner (adds Falsifier 3.5 Cut-size bridge)

Goal:
- Move from Option A ("S ~ log(chi) for fixed partition") toward Option B by
  varying partition geometry (subsystem sizes) so K_cut changes, then showing:
    slope_a(partition) ∝ K_cut(partition)

This is still a scoped toy model:
- S is generated by a simplified entropy generator (not from a Hamiltonian MERA)
- K_cut is computed by a declared proxy rule (not a true minimal cut in a realized network)

Outputs (SPEC v0.2 Claim 3 minimum + bridge detail):
outputs/exp3_claim3_v04_optionB/<run_id>/
  manifest.json
  raw_entropy.csv
  fits.json
  bound_checks.json
  bridge_checks.json
  verdict.json

CPU runnable. Deterministic under seed policy.
"""

from __future__ import annotations

import argparse
import csv
import datetime as dt
import json
import math
import os
import platform
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np


# ----------------------------
# Helpers
# ----------------------------

def run_id() -> str:
    t = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    r = os.urandom(4).hex()
    return f"{t}_{r}"


def write_json(path: Path, obj: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2)


def fit_linear(x: np.ndarray, y: np.ndarray) -> Tuple[float, float, float]:
    """
    Fit y = a*x + b; return (a, b, rss).
    """
    X = np.column_stack([x, np.ones_like(x)])
    coef, _, _, _ = np.linalg.lstsq(X, y, rcond=None)
    a, b = float(coef[0]), float(coef[1])
    yhat = X @ coef
    rss = float(np.sum((y - yhat) ** 2))
    return a, b, rss


def aic_bic(rss: float, n: int, k: int, eps: float = 1e-12) -> Tuple[float, float]:
    """
    Gaussian errors AIC/BIC from RSS.
    """
    rss = max(float(rss), eps)
    n = int(n)
    k = int(k)
    aic = n * math.log(rss / n) + 2 * k
    bic = n * math.log(rss / n) + k * math.log(n)
    return float(aic), float(bic)


def fit_log_power(log_chi: np.ndarray, y: np.ndarray, p_grid: np.ndarray) -> Dict[str, float]:
    """
    Fit y = a*(log_chi)^p + b with p selected by grid search.
    Uses AIC/BIC with k=3 (a,b,p).
    """
    best = {"p": float("nan"), "a": float("nan"), "b": float("nan"), "rss": float("inf"),
            "aic": float("inf"), "bic": float("inf")}
    n = len(y)
    for p in p_grid:
        z = np.power(log_chi, p)
        a, b, rss = fit_linear(z, y)
        aic, bic = aic_bic(rss, n=n, k=3)
        if rss < best["rss"]:
            best = {"p": float(p), "a": float(a), "b": float(b), "rss": float(rss),
                    "aic": float(aic), "bic": float(bic)}
    return best


def pearson_r(x: np.ndarray, y: np.ndarray) -> float:
    if len(x) < 2:
        return 0.0
    if float(np.std(x)) == 0.0 or float(np.std(y)) == 0.0:
        return 0.0
    return float(np.corrcoef(x, y)[0, 1])


# ----------------------------
# Partition + proxy K_cut
# ----------------------------

@dataclass(frozen=True)
class Partition:
    partition_id: str
    subsystem_size: int


def k_cut_proxy(N: int, A: int) -> int:
    """
    Declared proxy rule (SPEC v0.2 Claim 3 observables):
      K_cut = floor(log2(N/|A|)) + 1, min 1
    """
    ratio = max(1.0, float(N) / max(1, int(A)))
    return max(1, int(math.log(ratio, 2)) + 1)


# ----------------------------
# Simplified entropy generator (toy)
# ----------------------------

def generate_S(N: int, chi: int, part: Partition, seed: int, sigma: float = 0.05) -> Dict[str, float]:
    """
    Toy generator enforcing the cut-counting bound:
      S = K_cut * (log(chi) * multiplier), with multiplier in [0,1]

    The multiplier clipping is the "bound-holding contract":
      multiplier = clip(1 + noise, 0, 1)
    """
    rng = np.random.default_rng(seed)
    log_chi = float(np.log(int(chi)))
    K = int(k_cut_proxy(N, part.subsystem_size))

    noise = float(rng.normal(0.0, sigma))
    multiplier = max(0.0, min(1.0, 1.0 + noise))

    S = float(K * log_chi * multiplier)
    bound = float(K * log_chi)

    return {
        "S": S,
        "log_chi": log_chi,
        "K_cut": float(K),
        "bound": bound,
        "margin": float(bound - S),
        "noise_raw": noise,
        "multiplier": multiplier,
    }


# ----------------------------
# Falsifiers (SPEC v0.2)
# ----------------------------

def f31_monotonicity(medians_by_chi: Dict[int, float], chi_sweep: List[int]) -> Dict[str, object]:
    ok = True
    violations = []
    for c0, c1 in zip(chi_sweep, chi_sweep[1:]):
        s0 = medians_by_chi[c0]
        s1 = medians_by_chi[c1]
        if s1 < s0 - 1e-12:
            ok = False
            violations.append({"chi_prev": c0, "chi_next": c1, "S_prev": s0, "S_next": s1})
    return {"passed": bool(ok), "violations": violations}


def f32_replicate_robustness(slopes: List[float], cv_threshold: float = 0.10) -> Dict[str, object]:
    arr = np.array(slopes, dtype=float)
    mean = float(np.mean(arr))
    std = float(np.std(arr))
    cv = float(std / mean) if mean != 0 else float("inf")
    no_negative = bool(np.all(arr > 0.0))
    stable = bool(cv <= cv_threshold)
    return {
        "passed": bool(no_negative and stable),
        "no_negative_slopes": no_negative,
        "cv_threshold": float(cv_threshold),
        "slope_mean": mean,
        "slope_std": std,
        "slope_cv": cv,
        "slopes": [float(x) for x in arr.tolist()],
    }


def f33_model_selection(y_medians: np.ndarray, chi_sweep: List[int]) -> Dict[str, object]:
    chis = np.array(chi_sweep, dtype=float)
    logs = np.log(chis)

    # Model 1: S = a*log(chi) + b
    a1, b1, rss1 = fit_linear(logs, y_medians)
    aic1, bic1 = aic_bic(rss1, n=len(y_medians), k=2)

    # Model 2: S = a*chi + b
    a2, b2, rss2 = fit_linear(chis, y_medians)
    aic2, bic2 = aic_bic(rss2, n=len(y_medians), k=2)

    # Model 3: S = a*(log(chi))^p + b
    p_grid = np.linspace(0.5, 3.0, 251)
    m3 = fit_log_power(logs, y_medians, p_grid)

    competitor_aic = min(aic2, m3["aic"])
    competitor_bic = min(bic2, m3["bic"])

    delta_aic = float(competitor_aic - aic1)
    delta_bic = float(competitor_bic - bic1)

    passed = bool((a1 > 0.0) and (delta_aic >= 10.0) and (delta_bic >= 10.0))

    return {
        "passed": passed,
        "pass_condition": {"delta_aic_min": 10.0, "delta_bic_min": 10.0, "log_slope_positive": True},
        "delta_aic": delta_aic,
        "delta_bic": delta_bic,
        "model_1_log_linear": {"a": float(a1), "b": float(b1), "rss": float(rss1), "aic": float(aic1), "bic": float(bic1)},
        "model_2_linear_chi": {"a": float(a2), "b": float(b2), "rss": float(rss2), "aic": float(aic2), "bic": float(bic2)},
        "model_3_log_power": m3,
    }


def f34_bound_validity(bound_rows: List[Dict[str, object]]) -> Dict[str, object]:
    violations = [r for r in bound_rows if float(r["S"]) > float(r["bound"]) + 1e-12]
    return {"passed": bool(len(violations) == 0), "violation_count": int(len(violations)), "violations": violations[:10]}


def f35_cut_size_bridge(slope_means: Dict[str, float], kcuts: Dict[str, float],
                       r_threshold: float = 0.90, loo_threshold: float = 0.80) -> Dict[str, object]:
    """
    SPEC v0.2 Falsifier 3.5 (Option B):
    Across partitions, slope a_partition should be proportional to K_cut_partition.

    Pass condition:
      - Pearson r(slope_mean, K_cut) >= r_threshold
      - No single-partition dominance: leave-one-out r >= loo_threshold for all omissions
    """
    part_ids = sorted(slope_means.keys())
    x = np.array([kcuts[p] for p in part_ids], dtype=float)
    y = np.array([slope_means[p] for p in part_ids], dtype=float)

    r_all = pearson_r(x, y)

    loo = []
    for omit in part_ids:
        keep = [p for p in part_ids if p != omit]
        xx = np.array([kcuts[p] for p in keep], dtype=float)
        yy = np.array([slope_means[p] for p in keep], dtype=float)
        loo_r = pearson_r(xx, yy)
        loo.append({"omit": omit, "r": float(loo_r)})

    min_loo_r = float(min(d["r"] for d in loo)) if loo else 0.0

    passed = bool((r_all >= r_threshold) and (min_loo_r >= loo_threshold))

    return {
        "passed": passed,
        "thresholds": {"r_all_min": float(r_threshold), "leave_one_out_min": float(loo_threshold)},
        "r_all": float(r_all),
        "leave_one_out": loo,
        "min_leave_one_out_r": float(min_loo_r),
        "pairs": [{"partition_id": p, "K_cut": float(kcuts[p]), "slope_mean": float(slope_means[p])} for p in part_ids],
    }


# ----------------------------
# Runner
# ----------------------------

def main() -> None:
    ap = argparse.ArgumentParser(description="Claim 3 Option B runner (SPEC v0.2)")
    ap.add_argument("--output_root", default="outputs")
    ap.add_argument("--experiment_name", default="exp3_claim3_v04_optionB")
    ap.add_argument("--seed_base", type=int, default=42)
    ap.add_argument("--num_sites", type=int, default=64)
    ap.add_argument("--chi_sweep", default="2,3,4,6,8,12,16,24,32")
    ap.add_argument("--seeds_per_chi", type=int, default=7)

    # Partition geometry variation (subsystem sizes). Choose multiple to vary K_cut.
    ap.add_argument("--subsystem_sizes", default="32,16,8,4")

    # Falsifier thresholds
    ap.add_argument("--slope_cv_threshold", type=float, default=0.10)
    ap.add_argument("--bridge_r_threshold", type=float, default=0.90)
    ap.add_argument("--bridge_loo_threshold", type=float, default=0.80)

    args = ap.parse_args()

    out_root = Path(args.output_root)
    exp_name = str(args.experiment_name).strip()
    rid = run_id()
    out_dir = out_root / exp_name / rid
    out_dir.mkdir(parents=True, exist_ok=True)

    N = int(args.num_sites)

    chi_sweep = [int(x.strip()) for x in str(args.chi_sweep).split(",") if x.strip()]
    chi_sweep = sorted(list(dict.fromkeys(chi_sweep)))
    seeds_per_chi = int(args.seeds_per_chi)
    seed_base = int(args.seed_base)

    subsystem_sizes = [int(x.strip()) for x in str(args.subsystem_sizes).split(",") if x.strip()]
    subsystem_sizes = [max(1, min(N // 2, s)) for s in subsystem_sizes]  # keep sensible
    subsystem_sizes = sorted(list(dict.fromkeys(subsystem_sizes)), reverse=True)

    partitions = [
        Partition(partition_id=f"part_A_{A}", subsystem_size=int(A))
        for A in subsystem_sizes
    ]

    # Deterministic seed policy:
    # seed = seed_base + (partition_index*1_000_000) + (chi_index*1000) + rep
    def point_seed(p_idx: int, chi_idx: int, rep: int) -> int:
        return seed_base + p_idx * 1_000_000 + chi_idx * 1000 + rep

    raw_rows: List[Dict[str, object]] = []
    bound_rows: List[Dict[str, object]] = []

    # Per-partition summaries for falsifiers
    per_partition = {}

    for p_idx, part in enumerate(partitions):
        # Collect S values by chi,rep
        S_by_chi = {chi: [] for chi in chi_sweep}
        # Slopes per rep (fit across chi for each rep)
        slopes = []

        # Generate points
        for chi_idx, chi in enumerate(chi_sweep):
            for rep in range(seeds_per_chi):
                s = point_seed(p_idx, chi_idx, rep)
                g = generate_S(N=N, chi=chi, part=part, seed=s)

                raw_rows.append({
                    "chi": int(chi),
                    "seed": int(s),
                    "partition_id": part.partition_id,
                    "N": int(N),
                    "subsystem_size": int(part.subsystem_size),
                    "S": float(g["S"]),
                    "run_status": "ok",
                    "notes": "",
                })

                bound_rows.append({
                    "chi": int(chi),
                    "seed": int(s),
                    "partition_id": part.partition_id,
                    "S": float(g["S"]),
                    "K_cut": float(g["K_cut"]),
                    "log_chi": float(g["log_chi"]),
                    "bound": float(g["bound"]),
                    "margin": float(g["margin"]),
                    "passed": bool(g["S"] <= g["bound"] + 1e-12),
                })

                S_by_chi[chi].append(float(g["S"]))

        # Medians across seeds for each chi
        medians_by_chi = {chi: float(np.median(np.array(S_by_chi[chi], dtype=float))) for chi in chi_sweep}
        y_medians = np.array([medians_by_chi[chi] for chi in chi_sweep], dtype=float)

        # Slopes per replicate index
        logs = np.log(np.array(chi_sweep, dtype=float))
        for rep in range(seeds_per_chi):
            y_rep = []
            for chi_idx, chi in enumerate(chi_sweep):
                s = point_seed(p_idx, chi_idx, rep)
                match = [r for r in raw_rows if r["partition_id"] == part.partition_id and int(r["chi"]) == int(chi) and int(r["seed"]) == int(s)]
                y_rep.append(float(match[0]["S"]))
            y_rep = np.array(y_rep, dtype=float)
            a, b, rss = fit_linear(logs, y_rep)
            slopes.append(float(a))

        # Falsifiers per partition
        f31 = f31_monotonicity(medians_by_chi, chi_sweep)
        f32 = f32_replicate_robustness(slopes, cv_threshold=float(args.slope_cv_threshold))
        f33 = f33_model_selection(y_medians=y_medians, chi_sweep=chi_sweep)
        corr = pearson_r(np.log(np.array(chi_sweep, dtype=float)), y_medians)

        # K_cut for this partition (proxy)
        K = float(k_cut_proxy(N, part.subsystem_size))

        per_partition[part.partition_id] = {
            "partition": {"partition_id": part.partition_id, "subsystem_size": part.subsystem_size, "K_cut": K},
            "medians_by_chi": medians_by_chi,
            "corr_medians_log_chi": float(corr),
            "f31": f31,
            "f32": f32,
            "f33": f33,
            "slope_mean": float(f32["slope_mean"]),
            "slope_cv": float(f32["slope_cv"]),
        }

    # Global falsifier 3.4 bound validity across all points
    f34 = f34_bound_validity(bound_rows)

    # Bridge falsifier 3.5 across partitions
    slope_means = {pid: per_partition[pid]["slope_mean"] for pid in per_partition}
    kcuts = {pid: per_partition[pid]["partition"]["K_cut"] for pid in per_partition}
    f35 = f35_cut_size_bridge(
        slope_means=slope_means,
        kcuts=kcuts,
        r_threshold=float(args.bridge_r_threshold),
        loo_threshold=float(args.bridge_loo_threshold),
    )

    # Option B verdict mapping:
    # SUPPORTED if 3.1–3.5 pass across partitions (3.4 global)
    per_part_pass = all(
        per_partition[pid]["f31"]["passed"]
        and per_partition[pid]["f32"]["passed"]
        and per_partition[pid]["f33"]["passed"]
        for pid in per_partition
    )
    supported = bool(per_part_pass and f34["passed"] and f35["passed"])

    verdict = "SUPPORTED" if supported else ("REJECTED" if (not f34["passed"]) else "INCONCLUSIVE")

    # ----------------------------
    # Write artifacts (SPEC v0.2)
    # ----------------------------

    scope_limitations = [
        "This run validates scaling, model selection, bound compliance, and the cut-size bridge inside a simplified entropy generator and a proxy K_cut.",
        "It does not demonstrate that a full physical MERA simulation for a specified Hamiltonian reproduces the same scaling in the same regime, because S is not computed from a Hamiltonian ground state/time evolution and K_cut is not computed as an actual network minimal cut.",
    ]

    manifest = {
        "framework_spec": {"name": "FRAMEWORK_SPEC_v0.2", "version": "0.2.0", "date": "2026-02-25"},
        "experiment_name": exp_name,
        "run_id": rid,
        "timestamp_utc": dt.datetime.utcnow().isoformat() + "Z",
        "claim": {
            "claim_id": "C3",
            "option": "B",
            "statement": "Across multiple partitions, the fitted slope of S versus log chi scales with the partition-dependent cut proxy K_cut, consistent with minimal-cut intuition.",
        },
        "scope": {
            "fixed": {"N": N, "generator": "simplified (toy) entropy generator", "layout": "proxy cut-counting"},
            "varied": {"chi_sweep": chi_sweep, "seeds_per_chi": seeds_per_chi, "partitions": [p.partition_id for p in partitions]},
            "partition_definition": [{"partition_id": p.partition_id, "subsystem_size": p.subsystem_size} for p in partitions],
            "K_cut_proxy_rule": "K_cut = floor(log2(N/|A|)) + 1, min 1",
            "falsifiers_required": ["3.1", "3.2", "3.3", "3.4", "3.5"],
        },
        "seed_policy": {
            "seed_base": seed_base,
            "formula": "seed = seed_base + (partition_index*1_000_000) + (chi_index*1000) + rep",
        },
        "software": {"python": sys.version.split()[0], "numpy": np.__version__, "platform": platform.platform()},
        "outputs": {
            "manifest": "manifest.json",
            "raw_entropy": "raw_entropy.csv",
            "fits": "fits.json",
            "bound_checks": "bound_checks.json",
            "bridge_checks": "bridge_checks.json",
            "verdict": "verdict.json",
        },
        "scope_limitations": scope_limitations,
    }
    write_json(out_dir / "manifest.json", manifest)

    # raw_entropy.csv
    with (out_dir / "raw_entropy.csv").open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(
            f,
            fieldnames=["chi", "seed", "partition_id", "S", "run_status", "notes"],
        )
        w.writeheader()
        for r in raw_rows:
            w.writerow({
                "chi": int(r["chi"]),
                "seed": int(r["seed"]),
                "partition_id": r["partition_id"],
                "S": f"{float(r['S']):.10f}",
                "run_status": r["run_status"],
                "notes": r["notes"],
            })

    # fits.json (per partition)
    fits = {"chi_sweep": chi_sweep, "per_partition": {}}
    for pid, info in per_partition.items():
        fits["per_partition"][pid] = {
            "partition": info["partition"],
            "corr_medians_log_chi": info["corr_medians_log_chi"],
            "falsifier_3_3_model_selection": info["f33"],
            "slope_stats": {"mean": info["slope_mean"], "cv": info["slope_cv"]},
        }
    write_json(out_dir / "fits.json", fits)

    # bound_checks.json
    bound_checks = {
        "K_cut_proxy_rule": manifest["scope"]["K_cut_proxy_rule"],
        "summary": {
            "total_points": len(bound_rows),
            "violation_count": f34["violation_count"],
            "passed": f34["passed"],
            "min_margin": float(min(r["margin"] for r in bound_rows)) if bound_rows else float("nan"),
        },
        "rows": bound_rows,
    }
    write_json(out_dir / "bound_checks.json", bound_checks)

    # bridge_checks.json
    write_json(out_dir / "bridge_checks.json", f35)

    # verdict.json
    verdict_obj = {
        "framework_spec": {"version": "0.2.0"},
        "experiment_name": exp_name,
        "run_id": rid,
        "claim_id": "C3",
        "option": "B",
        "verdict": verdict,
        "scope_limitations": scope_limitations,
        "falsifiers": {
            "3.1_monotonicity_per_partition": {pid: per_partition[pid]["f31"] for pid in per_partition},
            "3.2_replicate_robustness_per_partition": {pid: per_partition[pid]["f32"] for pid in per_partition},
            "3.3_model_selection_per_partition": {pid: per_partition[pid]["f33"] for pid in per_partition},
            "3.4_bound_validity_global": f34,
            "3.5_cut_size_bridge": f35,
        },
        "summary": {
            "per_partition_passed_all_of_3_1_3_2_3_3": bool(per_part_pass),
            "bound_passed": bool(f34["passed"]),
            "bridge_passed": bool(f35["passed"]),
        },
        "artifact_paths": {
            "manifest": "manifest.json",
            "raw_entropy": "raw_entropy.csv",
            "fits": "fits.json",
            "bound_checks": "bound_checks.json",
            "bridge_checks": "bridge_checks.json",
            "verdict": "verdict.json",
        },
    }
    write_json(out_dir / "verdict.json", verdict_obj)

    # Minimal stdout for logs
    print(out_dir.as_posix())
    print(json.dumps({"verdict": verdict, "per_partition_pass": per_part_pass, "bound_pass": f34["passed"], "bridge_r": f35["r_all"]}))


if __name__ == "__main__":
    main()
